{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning for FrozenLake 4x4\n",
    "\n",
    "Based on https://colab.research.google.com/drive/1oqon14Iq8jzx6PhMJvja-mktFTru5GPl#scrollTo=5aQKQMJTJBPH"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ![image.png](attachment:image.png) -->\n",
    "\n",
    "The Frozen Lake environment specifications can be found [here](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "Frozen lake involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake.\n",
    "\n",
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "\n",
    "Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "\n",
    "The player makes moves until they reach the goal or fall in a hole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We start by importing the dependencies: Gymnasium, numpy, and random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and exploring the environment\n",
    "\n",
    "Even though the original problem description has a slippery environment, we are going to start working in a non-slippery environment. In it, if you go right, you only go right; in the original environment, if you intend to go right, you can go right, up or down with 1/3 probability.\n",
    "\n",
    "To make a deterministic Frozen Lake environment, we make use of the `is_slippery` flag. We'll also use `ansi`as rendering mode, as it is faster to visualize (the other option is `human`, which provides a graphical visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the environment on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space size and the state space size for this environment can be obtained as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "print(f'action size: {action_size}, state size: {state_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "We now proceed to implementing Q-learning and aplying it to our environment. In Q-learning, q-values are updated based on:\n",
    "\n",
    "$Q(S_{t},A_{t}) \\leftarrow Q(S_{t},A_{t}) + \\alpha [R_{t+1} + \\gamma \\max_{a} Q(S_{t+1},a) - Q(S_{t},A_{t})]$\n",
    "\n",
    "where $\\alpha$ is the learning rate (aka step size), and $\\gamma$ is the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q-table\n",
    "\n",
    "First, we create a Q-table of shape *state_size* x *action_size*, and initialize its values with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = # your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning hyperparameters\n",
    "\n",
    "Now we create some hyperparameters for the Q-learning algorithm:\n",
    "- the total number of episodes to run\n",
    "- the maximum number of steps per episode\n",
    "- the learning rate\n",
    "- the discount factor\n",
    "- the range for the exploration parameter epsilon\n",
    "- the epsilon decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 1000         # Total episodes\n",
    "max_steps = 100               # Max steps per episode\n",
    "\n",
    "learning_rate = 0.8           # Learning rate\n",
    "gamma = 0.95                  # Discounting factor\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability \n",
    "decay_rate = 0.001            # Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Q-learning algorithm\n",
    "\n",
    "Implement Q-learning and use it to train the agent, using an $\\epsilon$-greedy action selection with decreasing exploration probability. Print some information as learning takes place.\n",
    "Keep a list of the total rewards obtained after each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of rewards\n",
    "rewards = []\n",
    "\n",
    "# Iterate through the number of episodes\n",
    "for episode in range(total_episodes):\n",
    "    print(f\"episode: {episode}\")\n",
    "    \n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    #print(f\"state: {state}\")\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        # Choose an action (a) in the current world state (s)\n",
    "        \n",
    "        # Shall we explore or exploit?\n",
    "        ## generate a random number\n",
    "        exp_exp_tradeoff = random.uniform(0, 1)\n",
    "        \n",
    "        ## If this number > epsilon --> exploitation \n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            # taking the biggest Q value for this state\n",
    "            # your code here\n",
    "            \n",
    "        # Else explore randomly\n",
    "        else:\n",
    "            # your code here\n",
    "            \n",
    "            \n",
    "        # Take the action (a) and observe the outcome state (s') and reward (r)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Update Q(s,a) = Q(s,a) + alpha [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Update state\n",
    "        state = new_state\n",
    "        \n",
    "        # If done, finish episode\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    # reduce epsilon (because we need less and less exploration)\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    \n",
    "    rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cummulative reward obtained throughout the episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cumulative_sum = np.cumsum(rewards)\n",
    "plt.plot(cumulative_sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the resulting Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the optimal action for each state, according to the obtained Q-table values. All we need to do is to get the action with the maximum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.render())\n",
    "\n",
    "# Print the best action in every state\n",
    "#LEFT = 0 DOWN = 1 RIGHT = 2 UP = 3\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize, state by state, the outcome of following a greedy policy. For that, you just need to choose the best action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()[0]\n",
    "\n",
    "n_steps = 0\n",
    "terminated = False\n",
    "while terminated == False and n_steps < max_steps:\n",
    "    print(env.render())\n",
    "    # Take the action (index) that has the maximum expected future reward given the state\n",
    "    # your code here\n",
    "    \n",
    "\n",
    "    # Make a step to the next state\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    n_steps += 1\n",
    "\n",
    "print(env.render())\n",
    "env.close()\n",
    "print(n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a stochastic environment\n",
    "\n",
    "We now turn into using a slippery frozen lake environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True, render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the q-learning algorithm in this new environment, and compare the obtained results both in terms of the q-table, cummulative rewards, and the optimal action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try increasing the number of training episodes and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with an 8x8 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=True, render_mode=\"ansi\")\n",
    "#..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
